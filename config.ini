[Server]
log_file = logs/llmproxy.log
api_keys = config/api_keys.txt
port = 8000

[Backend_1]
url = http://wil-gpu-14.bluelobster.ai:10934
model = meta-llama/Meta-Llama-3.1-8B-Instruct
api_key = bl-2ny2hTassWhjy9oR7KTfZawk_um4L87UNW-opRj
queue_size = 10

[Backend_2]
url = http://wil-gpu-14.bluelobster.ai:10929
model = Qwen/Qwen2.5-72B-Instruct-GPTQ-Int4
api_key = bl-2ny2hTassWhjy9oR7KTfZawk_um4L87UNW-opRj
queue_size = 10

[Backend_3]
url = http://wil-gpu-14.bluelobster.ai:10910
model = mistralai/Mistral-Nemo-Instruct-2407
api_key = bl-2ny2hTassWhjy9oR7KTfZawk_um4L87UNW-opRj
queue_size = 10

[Backend_4]
url = http://98.115.241.69:47930
model = mistralai/Mistral-7B-Instruct-v0.2
api_key = token-abc123
queue_size = 10

[Backend_5]
url = http://98.115.241.69:48030
model = microsoft/Phi-3-mini-4k-instruct
api_key = token-abc123
queue_size = 10
